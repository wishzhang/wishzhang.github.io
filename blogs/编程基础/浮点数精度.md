---
title: 浮点数精度
date: 2021-09-21
tags: 

- 计算机组成原理

categories:

 - 编程基础
---

## 浮点数

由于十进制中的实数无法都准确换算成二进制数，如0.1，因此只能使用近似值的方式表达。由一个有效数字加上幂数这种方式表示的数值，称为浮点数。这种方式的设计，来自于对于值的表现范围和精密度之间的取舍。

在电脑使用的浮点数被规范化为IEEE 754标准——IEEE 二进制浮点数算术标准，大部分的编程语言都提供了相应的支持。

如下为32位单精度和64位双精度的表示法。

| 类型       | 符号位 | 指数 | 尾数（有效数字） |
| ---------- | ------ | ---- | ---------------- |
| 单精度32位 | 1位    | 8位  | 23位             |
| 双精度64位 | 1位    | 11位 | 52位             |


单精和双精浮点数的有效数字分别是有存储的23和52个位，加上最左手边没有存储的第1个位，即是24和53个位。

![331a974a25e9397a814585a925cbb6af.png](/assets/fudianshu.png)

由以上的计算，单精度和双精度浮点数可以保证7位和15位十进制有效数字（注意保证的意思是15位的任意十进制数都是安全的，而16位十进制数只是有些是安全的）。


## 是否 0.1+0.2 == 0.3

### 1  浮点数的存储
JavaScript的Number类型为双精度IEEE754 64位浮点类型。

0.1的二进制表示为0.00011001100110011……
可以表示为 1 * 2^-4 * 1.1001100110011，其中符号位为1，指数为2^-4 ，尾数为1.1001100110011
再用二进制科学技术法的方式具体点是：
V = (-1)^S * (1 + Fraction) * 2^E

这个E值可能是正数也可能是负数，那我们该怎么存储这个E呢？
我们这样解决，假如我们用 8 位字节来存储 E 这个数，如果只有正数的话，储存的值的范围是 0 ~ 254，而如果要储存正负数的话，值的范围就是 -127~127，我们在存储的时候，把要存储的数字加上 127，这样当我们存 -127 的时候，我们存 0，当存 127 的时候，存 254，这样就解决了存负数的问题。对应的，当取值的时候，我们再减去 127。


在这个标准下：我们会用 1 位存储 S，0 表示正数，1 表示负数。用 11 位存储 E + bias，对于 11 位来说，bias 的值是 2^(11-1) - 1，也就是 1023。用 52 位存储 Fraction。

举个例子，就拿 0.1 来看，对应二进制是 1 * 1.1001100110011…… * 2^-4， Sign 是 0，E + bias 是 -4 + 1023 = 1019，1019 用二进制表示是 1111111011，Fraction 是 1001100110011……
对应 64 个字节位的完整表示就是：
`0 01111111011 1001100110011001100110011001100110011001100110011010`

同理, 0.2 表示的完整表示是：
`0 01111111100 1001100110011001100110011001100110011001100110011010`
所以当 0.1 存下来的时候，就已经发生了精度丢失，当我们用浮点数进行运算的时候，使用的其实是精度丢失后的数。


### 2 浮点数的运算
关于浮点数的运算，一般由以下五个步骤完成：对阶、尾数运算、规格化、舍入处理、溢出判断。我们来简单看一下0.1和0.2的计算。

首先是对阶，所谓对阶，就是把阶码调整为相同，比如 0.1 是 1.1001100110011…… * 2^-4，阶码是 -4，
而 0.2 就是 1.10011001100110...* 2^-3，阶码是 -3，两个阶码不同，所以先调整为相同的阶码再进行计算，调整原则是小阶对大阶，也就是 0.1 的 -4 调整为 -3，对应变成 0.11001100110011…… * 2^-3

接下来是尾数计算:  
```
  0.1100110011001100110011001100110011001100110011001101
+ 1.1001100110011001100110011001100110011001100110011010
————————————————————————————————————————————————————————
 10.0110011001100110011001100110011001100110011001100111
```
我们得到结果为` 10.0110011001100110011001100110011001100110011001100111 * 2^-3`
将这个结果处理一下，即结果规格化，变成 
`1.0011001100110011001100110011001100110011001100110011(1) * 2^-2`

括号里的 1 意思是说计算后这个 1 超出了范围，所以要被舍弃了。再然后是舍入，四舍五入对应到二进制中，就是 0 舍 1 入，因为我们要把括号里的 1 丢了，所以这里会进一，结果变成`1.0011001100110011001100110011001100110011001100110100 * 2^-2`

本来还有一个溢出判断，因为这里不涉及，就不讲了。所以最终的结果存成 64 位就是
`0 01111111101 0011001100110011001100110011001100110011001100110100`
将它转换为10进制数就得到 `0.30000000000000004440892098500626`

因为两次存储时的精度丢失加上一次运算时的精度丢失，最终导致了 0.1 + 0.2 !== 0.3